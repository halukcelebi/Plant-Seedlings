{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project website: https://www.kaggle.com/competitions/plant-seedlings-classification/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "175b0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a51436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_into_rgb_array(directory, label_val):\n",
    "    import cv2\n",
    "    image_files = [file for file in os.listdir(directory) if file.endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
    "\n",
    "    images_array2 = []\n",
    "    label = []\n",
    "\n",
    "    for file_name in image_files:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                \n",
    "                # fixing directory path \n",
    "                str_file_pth = file_path.split(\"\\\\\")\n",
    "                file_path =  str_file_pth[0]+\"/\"+str_file_pth[1]                            \n",
    "                #print(\"file path: {}\".format(file_path))\n",
    "                \n",
    "                #read image\n",
    "                image = cv2.imread(file_path)\n",
    "                #print(image.shape)\n",
    "                # convert to image to array\n",
    "                img_array = Image.fromarray(image,'RGB')\n",
    "                #image sizes are not fix, in order not to have \n",
    "                # problem at CNN input, we need to have fix\n",
    "                # size of image size. Therefore, we resize \n",
    "                # the input\n",
    "                resize_img = img_array.resize((60,60))\n",
    "                images_array2.append(np.array(resize_img))\n",
    "                label.append(label_val)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return ( images_array2, label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43a44343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Subdirectory: Black-grass\n",
      "Index: 1, Subdirectory: Charlock\n",
      "Index: 2, Subdirectory: Cleavers\n",
      "Index: 3, Subdirectory: Common Chickweed\n",
      "Index: 4, Subdirectory: Common wheat\n",
      "Index: 5, Subdirectory: Fat Hen\n",
      "Index: 6, Subdirectory: Loose Silky-bent\n",
      "Index: 7, Subdirectory: Maize\n",
      "Index: 8, Subdirectory: Scentless Mayweed\n",
      "Index: 9, Subdirectory: Shepherds Purse\n",
      "Index: 10, Subdirectory: Small-flowered Cranesbill\n",
      "Index: 11, Subdirectory: Sugar beet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_folder = \"./train\"\n",
    "\n",
    "# Get the list of subdirectories in the train folder\n",
    "subdirectories = [d for d in os.listdir(train_folder) if os.path.isdir(os.path.join(train_folder, d))]\n",
    "\n",
    "# Print the list of subdirectories\n",
    "for i, subdir in enumerate(subdirectories):\n",
    "    print(f\"Index: {i}, Subdirectory: {subdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d572a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Subdirectory: Black-grass\n",
      "<class 'list'>\n",
      "Index: 1, Subdirectory: Charlock\n",
      "<class 'list'>\n",
      "Index: 2, Subdirectory: Cleavers\n",
      "<class 'list'>\n",
      "Index: 3, Subdirectory: Common Chickweed\n",
      "<class 'list'>\n",
      "Index: 4, Subdirectory: Common wheat\n",
      "<class 'list'>\n",
      "Index: 5, Subdirectory: Fat Hen\n",
      "<class 'list'>\n",
      "Index: 6, Subdirectory: Loose Silky-bent\n",
      "<class 'list'>\n",
      "Index: 7, Subdirectory: Maize\n",
      "<class 'list'>\n",
      "Index: 8, Subdirectory: Scentless Mayweed\n",
      "<class 'list'>\n",
      "Index: 9, Subdirectory: Shepherds Purse\n",
      "<class 'list'>\n",
      "Index: 10, Subdirectory: Small-flowered Cranesbill\n",
      "<class 'list'>\n",
      "Index: 11, Subdirectory: Sugar beet\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#X = [];\n",
    "#Y = [];\n",
    "\n",
    "X = np.zeros((0, 60, 60, 3))  # Assuming your images are of size 50x50x3\n",
    "Y = np.zeros((0,))  # Assuming your labels are 1D\n",
    "\n",
    "for i, subdir in enumerate(subdirectories):\n",
    "    print(f\"Index: {i}, Subdirectory: {subdir}\")\n",
    "    [ img, label ] = read_images_into_rgb_array(\"./train/\"+subdir, i)\n",
    "    \n",
    "    if len(X)==0 :\n",
    "        X = img;\n",
    "        Y = label;\n",
    "        \n",
    "    else: \n",
    "        X = np.concatenate((X, img), axis=0)\n",
    "        Y = np.concatenate((Y, label), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "451cb49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4750, 60, 60, 3), (4750,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "664745c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list to array for further processing\n",
    "# infected\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "247f5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.arange(X.shape[0])\n",
    "np.random.shuffle(s)\n",
    "X=X[s]\n",
    "Y=Y[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b47ac7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization \n",
    "X = X/255;\n",
    "Y = Y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "960719d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(np.unique(Y))\n",
    "len_data=len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4690023",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,x_test)=X[(int)(0.1*len_data):],X[:(int)(0.1*len_data)]\n",
    "(y_train,y_test)=Y[(int)(0.1*len_data):],Y[:(int)(0.1*len_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd350a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test=keras.utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d53d038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "# input layer size is equal to frame pixel size\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu', input_shape=(60, 60, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(256, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(200, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output for fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(layers.Dense(264, activation='relu'))\n",
    "model.add(layers.Dense(12, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51a32f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c11826d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "193/193 [==============================] - 14s 67ms/step - loss: 0.2765 - accuracy: 0.2173 - val_loss: 0.2375 - val_accuracy: 0.3294\n",
      "Epoch 2/30\n",
      "193/193 [==============================] - 13s 68ms/step - loss: 0.2076 - accuracy: 0.4211 - val_loss: 0.1789 - val_accuracy: 0.5421\n",
      "Epoch 3/30\n",
      "193/193 [==============================] - 14s 71ms/step - loss: 0.1638 - accuracy: 0.5711 - val_loss: 0.1560 - val_accuracy: 0.6028\n",
      "Epoch 4/30\n",
      "193/193 [==============================] - 14s 73ms/step - loss: 0.1338 - accuracy: 0.6805 - val_loss: 0.1225 - val_accuracy: 0.7173\n",
      "Epoch 5/30\n",
      "193/193 [==============================] - 15s 76ms/step - loss: 0.1139 - accuracy: 0.7320 - val_loss: 0.1047 - val_accuracy: 0.7547\n",
      "Epoch 6/30\n",
      "193/193 [==============================] - 14s 73ms/step - loss: 0.0989 - accuracy: 0.7726 - val_loss: 0.0950 - val_accuracy: 0.7921\n",
      "Epoch 7/30\n",
      "193/193 [==============================] - 15s 76ms/step - loss: 0.0881 - accuracy: 0.7993 - val_loss: 0.0962 - val_accuracy: 0.7757\n",
      "Epoch 8/30\n",
      "193/193 [==============================] - 15s 77ms/step - loss: 0.0756 - accuracy: 0.8295 - val_loss: 0.1290 - val_accuracy: 0.6822\n",
      "Epoch 9/30\n",
      "193/193 [==============================] - 14s 73ms/step - loss: 0.0659 - accuracy: 0.8622 - val_loss: 0.0892 - val_accuracy: 0.7687\n",
      "Epoch 10/30\n",
      "193/193 [==============================] - 14s 73ms/step - loss: 0.0582 - accuracy: 0.8695 - val_loss: 0.0818 - val_accuracy: 0.8061\n",
      "Epoch 11/30\n",
      "193/193 [==============================] - 14s 74ms/step - loss: 0.0486 - accuracy: 0.8978 - val_loss: 0.0801 - val_accuracy: 0.8154\n",
      "Epoch 12/30\n",
      "193/193 [==============================] - 15s 76ms/step - loss: 0.0436 - accuracy: 0.9088 - val_loss: 0.0795 - val_accuracy: 0.8178\n",
      "Epoch 13/30\n",
      "193/193 [==============================] - 14s 74ms/step - loss: 0.0354 - accuracy: 0.9262 - val_loss: 0.0859 - val_accuracy: 0.8248\n",
      "Epoch 14/30\n",
      "193/193 [==============================] - 14s 74ms/step - loss: 0.0334 - accuracy: 0.9350 - val_loss: 0.0980 - val_accuracy: 0.8131\n",
      "Epoch 15/30\n",
      "193/193 [==============================] - 15s 77ms/step - loss: 0.0294 - accuracy: 0.9428 - val_loss: 0.1067 - val_accuracy: 0.7687\n",
      "Epoch 16/30\n",
      "193/193 [==============================] - 15s 76ms/step - loss: 0.0254 - accuracy: 0.9532 - val_loss: 0.0999 - val_accuracy: 0.8248\n",
      "Epoch 17/30\n",
      "193/193 [==============================] - 17s 89ms/step - loss: 0.0206 - accuracy: 0.9623 - val_loss: 0.1126 - val_accuracy: 0.7967\n",
      "Epoch 18/30\n",
      "193/193 [==============================] - 19s 97ms/step - loss: 0.0207 - accuracy: 0.9639 - val_loss: 0.1151 - val_accuracy: 0.8084\n",
      "Epoch 19/30\n",
      "193/193 [==============================] - 19s 98ms/step - loss: 0.0183 - accuracy: 0.9701 - val_loss: 0.1166 - val_accuracy: 0.8131\n",
      "Epoch 20/30\n",
      "193/193 [==============================] - 18s 92ms/step - loss: 0.0205 - accuracy: 0.9618 - val_loss: 0.1199 - val_accuracy: 0.7944\n",
      "Epoch 21/30\n",
      "193/193 [==============================] - 18s 95ms/step - loss: 0.0122 - accuracy: 0.9813 - val_loss: 0.1290 - val_accuracy: 0.8107\n",
      "Epoch 22/30\n",
      "193/193 [==============================] - 18s 95ms/step - loss: 0.0102 - accuracy: 0.9841 - val_loss: 0.1313 - val_accuracy: 0.8061\n",
      "Epoch 23/30\n",
      "193/193 [==============================] - 19s 97ms/step - loss: 0.0095 - accuracy: 0.9891 - val_loss: 0.1485 - val_accuracy: 0.7874\n",
      "Epoch 24/30\n",
      "193/193 [==============================] - 19s 97ms/step - loss: 0.0121 - accuracy: 0.9844 - val_loss: 0.1246 - val_accuracy: 0.7991\n",
      "Epoch 25/30\n",
      "193/193 [==============================] - 19s 101ms/step - loss: 0.0053 - accuracy: 0.9927 - val_loss: 0.1368 - val_accuracy: 0.8248\n",
      "Epoch 26/30\n",
      "193/193 [==============================] - 19s 98ms/step - loss: 0.0017 - accuracy: 0.9987 - val_loss: 0.1531 - val_accuracy: 0.8435\n",
      "Epoch 27/30\n",
      "193/193 [==============================] - 18s 95ms/step - loss: 0.0059 - accuracy: 0.9940 - val_loss: 0.1774 - val_accuracy: 0.8248\n",
      "Epoch 28/30\n",
      "193/193 [==============================] - 19s 101ms/step - loss: 0.0215 - accuracy: 0.9722 - val_loss: 0.1434 - val_accuracy: 0.8084\n",
      "Epoch 29/30\n",
      "193/193 [==============================] - 19s 99ms/step - loss: 0.0072 - accuracy: 0.9930 - val_loss: 0.1502 - val_accuracy: 0.8294\n",
      "Epoch 30/30\n",
      "193/193 [==============================] - 18s 95ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1652 - val_accuracy: 0.8248\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=20, epochs=30, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4faa4d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 - 0s - loss: 0.1685 - accuracy: 0.8295 - 418ms/epoch - 28ms/step\n",
      "Test Accuracy: 0.8294736742973328\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99096f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
